{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ìœ ì‚¬ë„ ì±—ë´‡ ì„±ëŠ¥ í–¥ìƒ ê¸°ë²•\n",
        "ğŸ¤” í‚¤ì›Œë“œëŠ” ì¡ì•„ë‚´ë©´ì„œ ì§ˆë¬¸ì˜ ë§¥ë½ë„ ìºì¹˜í•˜ê³  ì‹¶ë‹¤<br>\n",
        "ğŸ’¡ BM25 + Embedding ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•´ ë³´ì!"
      ],
      "metadata": {
        "id": "CWqRJ-Qift-u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPW7PD1ldcZb",
        "outputId": "2c2e9e78-ed85-436e-8d53-e5597c9ebcfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.42.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.112.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.42.0-py3-none-any.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, rank_bm25, python-multipart, orjson, JPype1, h11, ffmpy, aiofiles, uvicorn, starlette, konlpy, httpcore, httpx, fastapi, gradio-client, sentence_transformers, gradio\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "Successfully installed JPype1-1.5.0 aiofiles-23.2.1 fastapi-0.112.2 ffmpy-0.4.0 gradio-4.42.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 konlpy-0.6.0 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.9 rank_bm25-0.2.2 ruff-0.6.3 semantic-version-2.10.0 sentence_transformers-3.0.1 starlette-0.38.2 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "pip install rank_bm25 konlpy sentence_transformers gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë°ì´í„° ë¶ˆëŸ¬ì™€ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±"
      ],
      "metadata": {
        "id": "40pPMPZVgPeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON íŒŒì¼ ê²½ë¡œ ì§€ì •\n",
        "file_path = 'science_data.json'\n",
        "\n",
        "# JSON íŒŒì¼ ì½ê¸°\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# ì§ˆë¬¸ê³¼ ë‹µë³€ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
        "questions = [item['instruction'] for item in data]\n",
        "answers = [item['output'] for item in data]\n",
        "\n",
        "print(\"ì´ ì§ˆë¬¸ ê°œìˆ˜:\", len(questions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XagxB1-edkdf",
        "outputId": "4b4c3554-34e4-4212-d641-adf1413c4089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ ì§ˆë¬¸ ê°œìˆ˜: 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BM25 ìƒìœ„ 3ê°œ ë‹µë³€ í™•ì¸"
      ],
      "metadata": {
        "id": "MCecPkrfgTth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from konlpy.tag import Okt\n",
        "import numpy as np\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í†µí•œ í•œêµ­ì–´ í† í°í™”\n",
        "okt = Okt()\n",
        "tokenized_answers = [okt.morphs(question) for question in answers]\n",
        "\n",
        "# BM25 ëª¨ë¸ ìƒì„±\n",
        "bm25 = BM25Okapi(tokenized_answers)\n",
        "\n",
        "# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥\n",
        "query = \"ì „ë„ê°€ ë­ì•¼?\"\n",
        "tokenized_query = okt.morphs(query)\n",
        "\n",
        "# BM25 ì ìˆ˜ ê³„ì‚° ë° ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì§ˆë¬¸ ì°¾ê¸°\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "# best_doc_idx = doc_scores.argmax()\n",
        "\n",
        "# ìƒìœ„ 3ê°œì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "top_3_indices = np.argsort(bm25_scores)[-3:][::-1]\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ì‚¬ìš©ì ì§ˆë¬¸:\", query)\n",
        "print(\"\\nìƒìœ„ 3ê°œ ì§ˆë¬¸ ë° ë‹µë³€:\")\n",
        "for idx in top_3_indices:\n",
        "    print(f\"ì§ˆë¬¸: {questions[idx]}\")\n",
        "    print(f\"ë‹µë³€: {answers[idx]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV6aavZgdkgE",
        "outputId": "1c049595-6ec3-4fb2-b8fb-e3cdf9288a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‚¬ìš©ì ì§ˆë¬¸: ì „ë„ê°€ ë­ì•¼?\n",
            "\n",
            "ìƒìœ„ 3ê°œ ì§ˆë¬¸ ë° ë‹µë³€:\n",
            "ì§ˆë¬¸: ì—´ì „ë‹¬ì˜ ì„¸ ê°€ì§€ ë°©ë²•ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: ì—´ì „ë‹¬ì˜ ì„¸ ê°€ì§€ ë°©ë²•ì€ ì „ë„, ëŒ€ë¥˜, ë³µì‚¬ì…ë‹ˆë‹¤. ì „ë„ëŠ” ì—´ì´ ë¬¼ì§ˆì„ í†µí•´ ì§ì ‘ ì „ë‹¬ë˜ëŠ” ê³¼ì •, ëŒ€ë¥˜ëŠ” ìœ ì²´ì˜ ì´ë™ì„ í†µí•´ ì—´ì´ ì „ë‹¬ë˜ëŠ” ê³¼ì •, ë³µì‚¬ëŠ” ì—´ì´ ì „ìê¸°íŒŒì˜ í˜•íƒœë¡œ ì „ë‹¬ë˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
            "\n",
            "ì§ˆë¬¸: ê¸°ì²´ì˜ ë°€ë„ì™€ ì˜¨ë„ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: ê¸°ì²´ì˜ ë°€ë„ëŠ” ì˜¨ë„ì— ë°˜ë¹„ë¡€í•©ë‹ˆë‹¤. ê¸°ì²´ì˜ ì˜¨ë„ê°€ ìƒìŠ¹í•˜ë©´ ê¸°ì²´ ë¶„ìë“¤ì´ ë” ë¹ ë¥´ê²Œ ì›€ì§ì—¬ ë¶€í”¼ê°€ ì»¤ì§€ë¯€ë¡œ ë°€ë„ê°€ ê°ì†Œí•©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ ì˜¨ë„ê°€ ë‚®ì•„ì§€ë©´ ë°€ë„ê°€ ì¦ê°€í•©ë‹ˆë‹¤.\n",
            "\n",
            "ì§ˆë¬¸: í™”í•™ì—ì„œ ë°˜ì‘ ì†ë„ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: í™”í•™ ë°˜ì‘ ì†ë„ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œë¡œëŠ” ì˜¨ë„, ë°˜ì‘ë¬¼ì˜ ë†ë„, ì´‰ë§¤, ì••ë ¥(ê¸°ì²´ ë°˜ì‘ì˜ ê²½ìš°) ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì˜¨ë„ê°€ ë†’ì•„ì§€ë©´ ë°˜ì‘ ì†ë„ê°€ ì¦ê°€í•˜ë©°, ë†ë„ê°€ ë†’ì„ìˆ˜ë¡ ë°˜ì‘ ì†ë„ê°€ ë¹¨ë¼ì§‘ë‹ˆë‹¤.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding ëª¨ë¸ ìƒìœ„ 3ê°œ ë‹µë³€ í™•ì¸"
      ],
      "metadata": {
        "id": "CGiROLnLgd4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Hugging Face ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# ì§ˆë¬¸ ì„ë² ë”© ìƒì„±\n",
        "answer_embeddings = model.encode(answers, convert_to_tensor=True)\n",
        "\n",
        "# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ë° ì„ë² ë”© ìƒì„±\n",
        "query = \"ì „ë„ê°€ ë­ì•¼?\"\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "cosine_scores = util.pytorch_cos_sim(query_embedding, answer_embeddings)\n",
        "\n",
        "# ìƒìœ„ 3ê°œì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "top_3_indices = torch.argsort(cosine_scores, descending=True)[0][:3].tolist()\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ì‚¬ìš©ì ì§ˆë¬¸:\", query)\n",
        "print(\"\\nìƒìœ„ 3ê°œ ì§ˆë¬¸ ë° ë‹µë³€:\")\n",
        "for idx in top_3_indices:\n",
        "    print(f\"ì§ˆë¬¸: {questions[idx]}\")\n",
        "    print(f\"ë‹µë³€: {answers[idx]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u6JVPZCdy1e",
        "outputId": "91dbc407-33b7-459e-ecb7-5eac9425d308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‚¬ìš©ì ì§ˆë¬¸: ì „ë„ê°€ ë­ì•¼?\n",
            "\n",
            "ìƒìœ„ 3ê°œ ì§ˆë¬¸ ë° ë‹µë³€:\n",
            "ì§ˆë¬¸: ì—´ì „ë‹¬ì˜ ì„¸ ê°€ì§€ ë°©ë²•ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: ì—´ì „ë‹¬ì˜ ì„¸ ê°€ì§€ ë°©ë²•ì€ ì „ë„, ëŒ€ë¥˜, ë³µì‚¬ì…ë‹ˆë‹¤. ì „ë„ëŠ” ì—´ì´ ë¬¼ì§ˆì„ í†µí•´ ì§ì ‘ ì „ë‹¬ë˜ëŠ” ê³¼ì •, ëŒ€ë¥˜ëŠ” ìœ ì²´ì˜ ì´ë™ì„ í†µí•´ ì—´ì´ ì „ë‹¬ë˜ëŠ” ê³¼ì •, ë³µì‚¬ëŠ” ì—´ì´ ì „ìê¸°íŒŒì˜ í˜•íƒœë¡œ ì „ë‹¬ë˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
            "\n",
            "ì§ˆë¬¸: ì „ê¸° íšŒë¡œì—ì„œ ì „ì••ì˜ ì—­í• ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: ì „ê¸° íšŒë¡œì—ì„œ ì „ì••ì€ ì „ë¥˜ë¥¼ íë¥´ê²Œ í•˜ëŠ” ì›ë™ë ¥ì…ë‹ˆë‹¤. ì „ì••ì€ ì „í•˜ë¥¼ ì „ë„ì²´ë¥¼ í†µí•´ ì´ë™ì‹œí‚¤ëŠ” í˜ì„ ì œê³µí•˜ë©°, ì „ê¸° íšŒë¡œì—ì„œ ì „ë¥˜ì˜ íë¦„ì„ ê²°ì •í•©ë‹ˆë‹¤.\n",
            "\n",
            "ì§ˆë¬¸: ì „ê¸° íšŒë¡œì—ì„œ ì „ë¥˜ì˜ ì •ì˜ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: ì „ë¥˜ëŠ” ì „í•˜ê°€ ì „ë„ì²´ë¥¼ í†µí•´ íë¥´ëŠ” ì†ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¬¼ë¦¬ëŸ‰ì…ë‹ˆë‹¤. ì „ë¥˜ì˜ ë‹¨ìœ„ëŠ” ì•”í˜ì–´(A)ì´ë©°, ì „ì••ì— ì˜í•´ ì „í•˜ê°€ ì´ë™í•˜ë©´ì„œ íšŒë¡œë¥¼ í†µí•´ íë¦…ë‹ˆë‹¤.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì•™ìƒë¸” ëª¨ë¸ ìƒìœ„ 3ê°œ ë‹µë³€ í™•ì¸"
      ],
      "metadata": {
        "id": "yIF83zZ4ghdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import torch\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# JSON íŒŒì¼ ê²½ë¡œ ì§€ì •\n",
        "file_path = 'science_data.json'\n",
        "\n",
        "# JSON íŒŒì¼ ì½ê¸°\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# ì§ˆë¬¸ê³¼ ë‹µë³€ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
        "questions = [item['instruction'] for item in data]\n",
        "answers = [item['output'] for item in data]\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í†µí•œ í•œêµ­ì–´ í† í°í™”\n",
        "okt = Okt()\n",
        "tokenized_answers = [okt.morphs(answer) for answer in answers]\n",
        "\n",
        "# BM25 ëª¨ë¸ ìƒì„±\n",
        "bm25 = BM25Okapi(tokenized_answers, k1=1.5, b=0.75)\n",
        "\n",
        "# Hugging Face ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# ë‹µë³€ ì„ë² ë”© ìƒì„±\n",
        "answer_embeddings = model.encode(answers, convert_to_tensor=True)\n",
        "\n",
        "# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥\n",
        "query = \"ì „ë„ê°€ ë­ì•¼?\"\n",
        "\n",
        "# BM25 ê²€ìƒ‰\n",
        "tokenized_query = okt.morphs(query)\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "# ì„ë² ë”© ê²€ìƒ‰\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "cosine_scores = util.pytorch_cos_sim(query_embedding, answer_embeddings).squeeze().tolist()\n",
        "\n",
        "# ì ìˆ˜ ê²°í•©\n",
        "alpha = 0.5\n",
        "combined_scores = [(alpha * bm25_score + (1 - alpha) * cosine_score) for bm25_score, cosine_score in zip(bm25_scores, cosine_scores)]\n",
        "\n",
        "# ìƒìœ„ 3ê°œì˜ ê²°ê³¼ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "top_3_indices = np.argsort(combined_scores)[::-1][:3]\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ê²€ìƒ‰ì–´:\", query)\n",
        "print(\"\\nìƒìœ„ 3ê°œ ì§ˆë¬¸ ë° ë‹µë³€:\")\n",
        "for idx in top_3_indices:\n",
        "    print(f\"ì§ˆë¬¸: {questions[idx]}\")\n",
        "    print(f\"ë‹µë³€: {answers[idx]}\")\n",
        "    print(f\"BM25 ì ìˆ˜: {bm25_scores[idx]}\")\n",
        "    print(f\"ì„ë² ë”© ìœ ì‚¬ë„: {cosine_scores[idx]}\")\n",
        "    print(f\"ê²°í•© ì ìˆ˜: {combined_scores[idx]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "degLk4PPd1Gs",
        "outputId": "e957aedc-aed3-4690-f3d6-ed0bfd7c58b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê²€ìƒ‰ì–´: ì „ë„ê°€ ë­ì•¼?\n",
            "\n",
            "ìƒìœ„ 3ê°œ ì§ˆë¬¸ ë° ë‹µë³€:\n",
            "ì§ˆë¬¸: ì—´ì „ë‹¬ì˜ ì„¸ ê°€ì§€ ë°©ë²•ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: ì—´ì „ë‹¬ì˜ ì„¸ ê°€ì§€ ë°©ë²•ì€ ì „ë„, ëŒ€ë¥˜, ë³µì‚¬ì…ë‹ˆë‹¤. ì „ë„ëŠ” ì—´ì´ ë¬¼ì§ˆì„ í†µí•´ ì§ì ‘ ì „ë‹¬ë˜ëŠ” ê³¼ì •, ëŒ€ë¥˜ëŠ” ìœ ì²´ì˜ ì´ë™ì„ í†µí•´ ì—´ì´ ì „ë‹¬ë˜ëŠ” ê³¼ì •, ë³µì‚¬ëŠ” ì—´ì´ ì „ìê¸°íŒŒì˜ í˜•íƒœë¡œ ì „ë‹¬ë˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
            "BM25 ì ìˆ˜: 5.245804106134422\n",
            "ì„ë² ë”© ìœ ì‚¬ë„: 0.38906535506248474\n",
            "ê²°í•© ì ìˆ˜: 2.8174347305984533\n",
            "\n",
            "ì§ˆë¬¸: ê¸°ì²´ì˜ ë°€ë„ì™€ ì˜¨ë„ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: ê¸°ì²´ì˜ ë°€ë„ëŠ” ì˜¨ë„ì— ë°˜ë¹„ë¡€í•©ë‹ˆë‹¤. ê¸°ì²´ì˜ ì˜¨ë„ê°€ ìƒìŠ¹í•˜ë©´ ê¸°ì²´ ë¶„ìë“¤ì´ ë” ë¹ ë¥´ê²Œ ì›€ì§ì—¬ ë¶€í”¼ê°€ ì»¤ì§€ë¯€ë¡œ ë°€ë„ê°€ ê°ì†Œí•©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ ì˜¨ë„ê°€ ë‚®ì•„ì§€ë©´ ë°€ë„ê°€ ì¦ê°€í•©ë‹ˆë‹¤.\n",
            "BM25 ì ìˆ˜: 1.6201333766383752\n",
            "ì„ë² ë”© ìœ ì‚¬ë„: 0.07426438480615616\n",
            "ê²°í•© ì ìˆ˜: 0.8471988807222657\n",
            "\n",
            "ì§ˆë¬¸: í™”í•™ì—ì„œ ë°˜ì‘ ì†ë„ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: í™”í•™ ë°˜ì‘ ì†ë„ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œë¡œëŠ” ì˜¨ë„, ë°˜ì‘ë¬¼ì˜ ë†ë„, ì´‰ë§¤, ì••ë ¥(ê¸°ì²´ ë°˜ì‘ì˜ ê²½ìš°) ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì˜¨ë„ê°€ ë†’ì•„ì§€ë©´ ë°˜ì‘ ì†ë„ê°€ ì¦ê°€í•˜ë©°, ë†ë„ê°€ ë†’ì„ìˆ˜ë¡ ë°˜ì‘ ì†ë„ê°€ ë¹¨ë¼ì§‘ë‹ˆë‹¤.\n",
            "BM25 ì ìˆ˜: 1.4968180981192423\n",
            "ì„ë² ë”© ìœ ì‚¬ë„: 0.1477956622838974\n",
            "ê²°í•© ì ìˆ˜: 0.8223068802015698\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì•™ìƒë¸” ëª¨ë¸ Gradio ì±—ë´‡ êµ¬í˜„\n",
        "â¡ ì •í™•ë„ëŠ” ë†’ì•„ì¡Œìœ¼ë‚˜ ë‹µë³€ ì¶œë ¥ ì†ë„ê°€ ëŠë¦¬ë‹¤ ğŸ˜‚"
      ],
      "metadata": {
        "id": "Hw3DVMl3gpES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from konlpy.tag import Okt\n",
        "from functools import lru_cache\n",
        "\n",
        "# ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
        "data_files = {\n",
        "    'êµ­ì–´': 'korean_data.json',\n",
        "    'ìˆ˜í•™': 'math_data.json',\n",
        "    'ì˜ì–´': 'english_data.json',\n",
        "    'ì‚¬íšŒ/ì—­ì‚¬': 'social_data.json',\n",
        "    'ê³¼í•™': 'science_data.json'\n",
        "}\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„ê¸°ì™€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "okt = Okt()\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "@lru_cache(maxsize = None)\n",
        "def load_data(topic):\n",
        "    # ì„ íƒëœ ì£¼ì œì— ë§ëŠ” ë°ì´í„° íŒŒì¼ ë¡œë“œ\n",
        "    file_path = data_files.get(topic)\n",
        "    if not file_path:\n",
        "        return [], [], [], []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    questions = [item['instruction'] for item in data]\n",
        "    answers = [item['output'] for item in data]\n",
        "\n",
        "    # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ í•œêµ­ì–´ í† í°í™”\n",
        "    tokenized_answers = [okt.morphs(answer) for answer in answers]\n",
        "\n",
        "    # BM25 ëª¨ë¸ ìƒì„±\n",
        "    bm25 = BM25Okapi(tokenized_answers, k1=1.5, b=0.75)\n",
        "\n",
        "    # ë‹µë³€ ì„ë² ë”© ìƒì„±\n",
        "    answer_embeddings = model.encode(answers, convert_to_tensor=True)\n",
        "\n",
        "    return questions, answers, bm25, answer_embeddings\n",
        "\n",
        "def get_answer(query, topic):\n",
        "    questions, answers, bm25, answer_embeddings = load_data(topic)\n",
        "    if not questions:\n",
        "        return \"ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì£¼ì œë¥¼ ì„ íƒí•˜ì„¸ìš”.\"\n",
        "\n",
        "    # BM25 ê²€ìƒ‰\n",
        "    tokenized_query = okt.morphs(query)\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # ì„ë² ë”© ê²€ìƒ‰\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    cosine_scores = util.pytorch_cos_sim(query_embedding, answer_embeddings).squeeze().tolist()\n",
        "\n",
        "    # ì ìˆ˜ ê²°í•©\n",
        "    alpha = 0.5\n",
        "    combined_scores = [(alpha * bm25_score + (1 - alpha) * cosine_score)\n",
        "                       for bm25_score, cosine_score in zip(bm25_scores, cosine_scores)]\n",
        "\n",
        "    # ìƒìœ„ 3ê°œì˜ ê²°ê³¼ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "    top_3_indices = np.argsort(combined_scores)[::-1][:3]\n",
        "\n",
        "    # ê²°ê³¼ êµ¬ì„±\n",
        "    results = []\n",
        "    for idx in top_3_indices:\n",
        "        results.append({\n",
        "            \"ì§ˆë¬¸\": questions[idx],\n",
        "            \"ë‹µë³€\": answers[idx],\n",
        "            \"BM25 ì ìˆ˜\": bm25_scores[idx],\n",
        "            \"ì„ë² ë”© ìœ ì‚¬ë„\": cosine_scores[idx],\n",
        "            \"ê²°í•© ì ìˆ˜\": combined_scores[idx]\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
        "def gradio_interface(query, topic):\n",
        "    results = get_answer(query, topic)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return \"\\n\".join([f\"ì§ˆë¬¸: {result['ì§ˆë¬¸']}\\në‹µë³€: {result['ë‹µë³€']}\\nBM25 ì ìˆ˜: {result['BM25 ì ìˆ˜']}\\nì„ë² ë”© ìœ ì‚¬ë„: {result['ì„ë² ë”© ìœ ì‚¬ë„']}\\nê²°í•© ì ìˆ˜: {result['ê²°í•© ì ìˆ˜']}\"\n",
        "                       for result in results])\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"ì§ˆë¬¸\", placeholder=\"ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.\"),\n",
        "        gr.Dropdown(choices=list(data_files.keys()), label=\"ì£¼ì œ ì„ íƒ\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"í•™ìŠµ ì±—ë´‡\",\n",
        "    description=\"ê³¼ëª©ì„ ì„ íƒí•˜ê³  ê¶ê¸ˆí•œ ê±¸ ë¬¼ì–´ë³´ì„¸ìš”!\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "0F42oPkNd2zZ",
        "outputId": "cfc854a7-4416-4b52-a9d0-5d7c26eb91bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ea4502cf1ba1161d7d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ea4502cf1ba1161d7d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì•™ìƒë¸” ëª¨ë¸ Gradio ì±—ë´‡ êµ¬í˜„ (pkl íŒŒì¼ë¡œ ì†ë„ ë†’ì¸ ë²„ì „)"
      ],
      "metadata": {
        "id": "Q9uwWI7Dg2iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
        "data_files = {\n",
        "    'êµ­ì–´': 'korean_data.json',\n",
        "    'ìˆ˜í•™': 'math_data.json',\n",
        "    'ì˜ì–´': 'english_data.json',\n",
        "    'ì‚¬íšŒì—­ì‚¬': 'social_data.json',\n",
        "    'ê³¼í•™': 'science_data.json'\n",
        "}\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„ê¸°ì™€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "okt = Okt()\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "def save_pickle_file(topic):\n",
        "    # ì„ íƒëœ ì£¼ì œì— ë§ëŠ” ë°ì´í„° íŒŒì¼ ë¡œë“œ\n",
        "    file_path = data_files.get(topic)\n",
        "    if not file_path:\n",
        "        print(f\"{topic}ì˜ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    questions = [item['instruction'] for item in data]\n",
        "    answers = [item['output'] for item in data]\n",
        "\n",
        "    # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ í•œêµ­ì–´ í† í°í™”\n",
        "    tokenized_answers = [okt.morphs(answer) for answer in answers]\n",
        "\n",
        "    # BM25 ëª¨ë¸ ìƒì„±\n",
        "    bm25 = BM25Okapi(tokenized_answers, k1=1.5, b=0.75)\n",
        "\n",
        "    # ë‹µë³€ ì„ë² ë”© ìƒì„±\n",
        "    answer_embeddings = model.encode(answers, convert_to_tensor=True)\n",
        "\n",
        "    # í”¼í´ íŒŒì¼ë¡œ ì €ì¥\n",
        "    pickle_file_path = f\"{topic}_data.pkl\"\n",
        "    with open(pickle_file_path, 'wb') as f:\n",
        "        pickle.dump((questions, answers, bm25, answer_embeddings), f)\n",
        "\n",
        "    print(f\"{topic}ì˜ í”¼í´ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {pickle_file_path}\")\n",
        "\n",
        "# ëª¨ë“  ì£¼ì œì— ëŒ€í•´ í”¼í´ íŒŒì¼ ì €ì¥\n",
        "for topic in data_files.keys():\n",
        "    save_pickle_file(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UXakIgnePmU",
        "outputId": "aea2aa22-6dbd-401b-d34b-be2f630b9f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "êµ­ì–´ì˜ í”¼í´ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: êµ­ì–´_data.pkl\n",
            "ìˆ˜í•™ì˜ í”¼í´ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ìˆ˜í•™_data.pkl\n",
            "ì˜ì–´ì˜ í”¼í´ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ì˜ì–´_data.pkl\n",
            "ì‚¬íšŒì—­ì‚¬ì˜ í”¼í´ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ì‚¬íšŒì—­ì‚¬_data.pkl\n",
            "ê³¼í•™ì˜ í”¼í´ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ê³¼í•™_data.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from konlpy.tag import Okt, Komoran\n",
        "from functools import lru_cache\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„ê¸°ì™€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "# okt = Okt()\n",
        "komoran = Komoran()\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def load_data(topic):\n",
        "    pickle_file_path = f\"{topic}_data.pkl\"\n",
        "    print(f\"Trying to load: {pickle_file_path}\")\n",
        "    try:\n",
        "        with open(pickle_file_path, 'rb') as f:\n",
        "            questions, answers, bm25, answer_embeddings = pickle.load(f)\n",
        "        return questions, answers, bm25, answer_embeddings\n",
        "    except FileNotFoundError:\n",
        "        return [], [], [], []\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return [], [], [], []\n",
        "def get_answer(topic, query):\n",
        "    questions, answers, bm25, answer_embeddings = load_data(topic)\n",
        "    if not questions:\n",
        "        return \"ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì£¼ì œë¥¼ ì„ íƒí•˜ì„¸ìš”.\"\n",
        "\n",
        "    tokenized_answers = [komoran.morphs(answer) for answer in answers]\n",
        "    tokenized_query = komoran.morphs(query)\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # ì„ë² ë”© ê²€ìƒ‰\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    cosine_scores = util.pytorch_cos_sim(query_embedding, answer_embeddings).squeeze().tolist()\n",
        "\n",
        "    # ì ìˆ˜ ê²°í•©\n",
        "    alpha = 0.5\n",
        "    combined_scores = [(alpha * bm25_score + (1 - alpha) * cosine_score)\n",
        "                       for bm25_score, cosine_score in zip(bm25_scores, cosine_scores)]\n",
        "\n",
        "    # ìƒìœ„ 3ê°œì˜ ê²°ê³¼ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "    top_3_indices = np.argsort(combined_scores)[::-1][:3]\n",
        "\n",
        "    # ê²°ê³¼ êµ¬ì„±\n",
        "    results = []\n",
        "    for idx in top_3_indices:\n",
        "        results.append({\n",
        "            \"ì§ˆë¬¸\": questions[idx],\n",
        "            \"ë‹µë³€\": answers[idx],\n",
        "            \"BM25 ì ìˆ˜\": bm25_scores[idx],\n",
        "            \"ì„ë² ë”© ìœ ì‚¬ë„\": cosine_scores[idx],\n",
        "            \"ê²°í•© ì ìˆ˜\": combined_scores[idx]\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
        "def gradio_interface(topic, query):\n",
        "    results = get_answer(topic, query)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return \"\\n\".join([f\"ì§ˆë¬¸: {result['ì§ˆë¬¸']}\\në‹µë³€: {result['ë‹µë³€']}\\nBM25 ì ìˆ˜: {result['BM25 ì ìˆ˜']}\\nì„ë² ë”© ìœ ì‚¬ë„: {result['ì„ë² ë”© ìœ ì‚¬ë„']}\\nê²°í•© ì ìˆ˜: {result['ê²°í•© ì ìˆ˜']}\"\n",
        "                       for result in results])\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=list(data_files.keys()), label=\"ê³¼ëª© ì„ íƒ\"),\n",
        "        gr.Textbox(label=\"ì§ˆë¬¸\", placeholder=\"ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"í•™ìŠµ ì±—ë´‡\",\n",
        "    description=\"ê³¼ëª©ì„ ì„ íƒí•˜ê³  ê¶ê¸ˆí•œ ê±¸ ë¬¼ì–´ë³´ì„¸ìš”!\"\n",
        ")\n",
        "\n",
        "iface.launch(debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "EtcsoluQeViz",
        "outputId": "39d79953-23f9-4877-9eee-5936dd3df081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://ab0e6a8176289b2143.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ab0e6a8176289b2143.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to load: ìˆ˜í•™_data.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(io.BytesIO(b))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://ea4502cf1ba1161d7d.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://ab0e6a8176289b2143.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}