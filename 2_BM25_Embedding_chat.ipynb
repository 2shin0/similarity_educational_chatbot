{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 유사도 챗봇 성능 향상 기법\n",
        "🤔 키워드는 잡아내면서 질문의 맥락도 캐치하고 싶다<br>\n",
        "💡 BM25 + Embedding 모델을 결합하여 유사도를 측정해 보자!"
      ],
      "metadata": {
        "id": "CWqRJ-Qift-u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPW7PD1ldcZb",
        "outputId": "2c2e9e78-ed85-436e-8d53-e5597c9ebcfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.42.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.112.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.42.0-py3-none-any.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, rank_bm25, python-multipart, orjson, JPype1, h11, ffmpy, aiofiles, uvicorn, starlette, konlpy, httpcore, httpx, fastapi, gradio-client, sentence_transformers, gradio\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "Successfully installed JPype1-1.5.0 aiofiles-23.2.1 fastapi-0.112.2 ffmpy-0.4.0 gradio-4.42.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 konlpy-0.6.0 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.9 rank_bm25-0.2.2 ruff-0.6.3 semantic-version-2.10.0 sentence_transformers-3.0.1 starlette-0.38.2 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "pip install rank_bm25 konlpy sentence_transformers gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 불러와서 리스트 생성"
      ],
      "metadata": {
        "id": "40pPMPZVgPeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON 파일 경로 지정\n",
        "file_path = 'science_data.json'\n",
        "\n",
        "# JSON 파일 읽기\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# 질문과 답변 리스트 생성\n",
        "questions = [item['instruction'] for item in data]\n",
        "answers = [item['output'] for item in data]\n",
        "\n",
        "print(\"총 질문 개수:\", len(questions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XagxB1-edkdf",
        "outputId": "4b4c3554-34e4-4212-d641-adf1413c4089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 질문 개수: 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BM25 상위 3개 답변 확인"
      ],
      "metadata": {
        "id": "MCecPkrfgTth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from konlpy.tag import Okt\n",
        "import numpy as np\n",
        "\n",
        "# 형태소 분석기를 통한 한국어 토큰화\n",
        "okt = Okt()\n",
        "tokenized_answers = [okt.morphs(question) for question in answers]\n",
        "\n",
        "# BM25 모델 생성\n",
        "bm25 = BM25Okapi(tokenized_answers)\n",
        "\n",
        "# 사용자 질문 입력\n",
        "query = \"전도가 뭐야?\"\n",
        "tokenized_query = okt.morphs(query)\n",
        "\n",
        "# BM25 점수 계산 및 가장 관련성 높은 질문 찾기\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "# best_doc_idx = doc_scores.argmax()\n",
        "\n",
        "# 상위 3개의 인덱스 찾기\n",
        "top_3_indices = np.argsort(bm25_scores)[-3:][::-1]\n",
        "\n",
        "# 결과 출력\n",
        "print(\"사용자 질문:\", query)\n",
        "print(\"\\n상위 3개 질문 및 답변:\")\n",
        "for idx in top_3_indices:\n",
        "    print(f\"질문: {questions[idx]}\")\n",
        "    print(f\"답변: {answers[idx]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV6aavZgdkgE",
        "outputId": "1c049595-6ec3-4fb2-b8fb-e3cdf9288a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용자 질문: 전도가 뭐야?\n",
            "\n",
            "상위 3개 질문 및 답변:\n",
            "질문: 열전달의 세 가지 방법을 설명해 주세요.\n",
            "답변: 열전달의 세 가지 방법은 전도, 대류, 복사입니다. 전도는 열이 물질을 통해 직접 전달되는 과정, 대류는 유체의 이동을 통해 열이 전달되는 과정, 복사는 열이 전자기파의 형태로 전달되는 과정입니다.\n",
            "\n",
            "질문: 기체의 밀도와 온도 사이의 관계를 설명해 주세요.\n",
            "답변: 기체의 밀도는 온도에 반비례합니다. 기체의 온도가 상승하면 기체 분자들이 더 빠르게 움직여 부피가 커지므로 밀도가 감소합니다. 반대로 온도가 낮아지면 밀도가 증가합니다.\n",
            "\n",
            "질문: 화학에서 반응 속도에 영향을 미치는 요소를 설명해 주세요.\n",
            "답변: 화학 반응 속도에 영향을 미치는 요소로는 온도, 반응물의 농도, 촉매, 압력(기체 반응의 경우) 등이 있습니다. 온도가 높아지면 반응 속도가 증가하며, 농도가 높을수록 반응 속도가 빨라집니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 모델 상위 3개 답변 확인"
      ],
      "metadata": {
        "id": "CGiROLnLgd4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Hugging Face 임베딩 모델 로드\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# 질문 임베딩 생성\n",
        "answer_embeddings = model.encode(answers, convert_to_tensor=True)\n",
        "\n",
        "# 사용자 질문 입력 및 임베딩 생성\n",
        "query = \"전도가 뭐야?\"\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "cosine_scores = util.pytorch_cos_sim(query_embedding, answer_embeddings)\n",
        "\n",
        "# 상위 3개의 인덱스 찾기\n",
        "top_3_indices = torch.argsort(cosine_scores, descending=True)[0][:3].tolist()\n",
        "\n",
        "# 결과 출력\n",
        "print(\"사용자 질문:\", query)\n",
        "print(\"\\n상위 3개 질문 및 답변:\")\n",
        "for idx in top_3_indices:\n",
        "    print(f\"질문: {questions[idx]}\")\n",
        "    print(f\"답변: {answers[idx]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u6JVPZCdy1e",
        "outputId": "91dbc407-33b7-459e-ecb7-5eac9425d308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용자 질문: 전도가 뭐야?\n",
            "\n",
            "상위 3개 질문 및 답변:\n",
            "질문: 열전달의 세 가지 방법을 설명해 주세요.\n",
            "답변: 열전달의 세 가지 방법은 전도, 대류, 복사입니다. 전도는 열이 물질을 통해 직접 전달되는 과정, 대류는 유체의 이동을 통해 열이 전달되는 과정, 복사는 열이 전자기파의 형태로 전달되는 과정입니다.\n",
            "\n",
            "질문: 전기 회로에서 전압의 역할을 설명해 주세요.\n",
            "답변: 전기 회로에서 전압은 전류를 흐르게 하는 원동력입니다. 전압은 전하를 전도체를 통해 이동시키는 힘을 제공하며, 전기 회로에서 전류의 흐름을 결정합니다.\n",
            "\n",
            "질문: 전기 회로에서 전류의 정의를 설명해 주세요.\n",
            "답변: 전류는 전하가 전도체를 통해 흐르는 속도를 나타내는 물리량입니다. 전류의 단위는 암페어(A)이며, 전압에 의해 전하가 이동하면서 회로를 통해 흐릅니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 앙상블 모델 상위 3개 답변 확인"
      ],
      "metadata": {
        "id": "yIF83zZ4ghdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import torch\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# JSON 파일 경로 지정\n",
        "file_path = 'science_data.json'\n",
        "\n",
        "# JSON 파일 읽기\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# 질문과 답변 리스트 생성\n",
        "questions = [item['instruction'] for item in data]\n",
        "answers = [item['output'] for item in data]\n",
        "\n",
        "# 형태소 분석기를 통한 한국어 토큰화\n",
        "okt = Okt()\n",
        "tokenized_answers = [okt.morphs(answer) for answer in answers]\n",
        "\n",
        "# BM25 모델 생성\n",
        "bm25 = BM25Okapi(tokenized_answers, k1=1.5, b=0.75)\n",
        "\n",
        "# Hugging Face 임베딩 모델 로드\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# 답변 임베딩 생성\n",
        "answer_embeddings = model.encode(answers, convert_to_tensor=True)\n",
        "\n",
        "# 사용자 질문 입력\n",
        "query = \"전도가 뭐야?\"\n",
        "\n",
        "# BM25 검색\n",
        "tokenized_query = okt.morphs(query)\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "# 임베딩 검색\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "cosine_scores = util.pytorch_cos_sim(query_embedding, answer_embeddings).squeeze().tolist()\n",
        "\n",
        "# 점수 결합\n",
        "alpha = 0.5\n",
        "combined_scores = [(alpha * bm25_score + (1 - alpha) * cosine_score) for bm25_score, cosine_score in zip(bm25_scores, cosine_scores)]\n",
        "\n",
        "# 상위 3개의 결과 인덱스 찾기\n",
        "top_3_indices = np.argsort(combined_scores)[::-1][:3]\n",
        "\n",
        "# 결과 출력\n",
        "print(\"검색어:\", query)\n",
        "print(\"\\n상위 3개 질문 및 답변:\")\n",
        "for idx in top_3_indices:\n",
        "    print(f\"질문: {questions[idx]}\")\n",
        "    print(f\"답변: {answers[idx]}\")\n",
        "    print(f\"BM25 점수: {bm25_scores[idx]}\")\n",
        "    print(f\"임베딩 유사도: {cosine_scores[idx]}\")\n",
        "    print(f\"결합 점수: {combined_scores[idx]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "degLk4PPd1Gs",
        "outputId": "e957aedc-aed3-4690-f3d6-ed0bfd7c58b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검색어: 전도가 뭐야?\n",
            "\n",
            "상위 3개 질문 및 답변:\n",
            "질문: 열전달의 세 가지 방법을 설명해 주세요.\n",
            "답변: 열전달의 세 가지 방법은 전도, 대류, 복사입니다. 전도는 열이 물질을 통해 직접 전달되는 과정, 대류는 유체의 이동을 통해 열이 전달되는 과정, 복사는 열이 전자기파의 형태로 전달되는 과정입니다.\n",
            "BM25 점수: 5.245804106134422\n",
            "임베딩 유사도: 0.38906535506248474\n",
            "결합 점수: 2.8174347305984533\n",
            "\n",
            "질문: 기체의 밀도와 온도 사이의 관계를 설명해 주세요.\n",
            "답변: 기체의 밀도는 온도에 반비례합니다. 기체의 온도가 상승하면 기체 분자들이 더 빠르게 움직여 부피가 커지므로 밀도가 감소합니다. 반대로 온도가 낮아지면 밀도가 증가합니다.\n",
            "BM25 점수: 1.6201333766383752\n",
            "임베딩 유사도: 0.07426438480615616\n",
            "결합 점수: 0.8471988807222657\n",
            "\n",
            "질문: 화학에서 반응 속도에 영향을 미치는 요소를 설명해 주세요.\n",
            "답변: 화학 반응 속도에 영향을 미치는 요소로는 온도, 반응물의 농도, 촉매, 압력(기체 반응의 경우) 등이 있습니다. 온도가 높아지면 반응 속도가 증가하며, 농도가 높을수록 반응 속도가 빨라집니다.\n",
            "BM25 점수: 1.4968180981192423\n",
            "임베딩 유사도: 0.1477956622838974\n",
            "결합 점수: 0.8223068802015698\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 앙상블 모델 Gradio 챗봇 구현\n",
        "➡ 정확도는 높아졌으나 답변 출력 속도가 느리다 😂"
      ],
      "metadata": {
        "id": "Hw3DVMl3gpES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from konlpy.tag import Okt\n",
        "from functools import lru_cache\n",
        "\n",
        "# 데이터 파일 경로 설정\n",
        "data_files = {\n",
        "    '국어': 'korean_data.json',\n",
        "    '수학': 'math_data.json',\n",
        "    '영어': 'english_data.json',\n",
        "    '사회/역사': 'social_data.json',\n",
        "    '과학': 'science_data.json'\n",
        "}\n",
        "\n",
        "# 형태소 분석기와 임베딩 모델 로드\n",
        "okt = Okt()\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "@lru_cache(maxsize = None)\n",
        "def load_data(topic):\n",
        "    # 선택된 주제에 맞는 데이터 파일 로드\n",
        "    file_path = data_files.get(topic)\n",
        "    if not file_path:\n",
        "        return [], [], [], []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    questions = [item['instruction'] for item in data]\n",
        "    answers = [item['output'] for item in data]\n",
        "\n",
        "    # 형태소 분석을 통한 한국어 토큰화\n",
        "    tokenized_answers = [okt.morphs(answer) for answer in answers]\n",
        "\n",
        "    # BM25 모델 생성\n",
        "    bm25 = BM25Okapi(tokenized_answers, k1=1.5, b=0.75)\n",
        "\n",
        "    # 답변 임베딩 생성\n",
        "    answer_embeddings = model.encode(answers, convert_to_tensor=True)\n",
        "\n",
        "    return questions, answers, bm25, answer_embeddings\n",
        "\n",
        "def get_answer(query, topic):\n",
        "    questions, answers, bm25, answer_embeddings = load_data(topic)\n",
        "    if not questions:\n",
        "        return \"데이터를 로드할 수 없습니다. 주제를 선택하세요.\"\n",
        "\n",
        "    # BM25 검색\n",
        "    tokenized_query = okt.morphs(query)\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # 임베딩 검색\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    cosine_scores = util.pytorch_cos_sim(query_embedding, answer_embeddings).squeeze().tolist()\n",
        "\n",
        "    # 점수 결합\n",
        "    alpha = 0.5\n",
        "    combined_scores = [(alpha * bm25_score + (1 - alpha) * cosine_score)\n",
        "                       for bm25_score, cosine_score in zip(bm25_scores, cosine_scores)]\n",
        "\n",
        "    # 상위 3개의 결과 인덱스 찾기\n",
        "    top_3_indices = np.argsort(combined_scores)[::-1][:3]\n",
        "\n",
        "    # 결과 구성\n",
        "    results = []\n",
        "    for idx in top_3_indices:\n",
        "        results.append({\n",
        "            \"질문\": questions[idx],\n",
        "            \"답변\": answers[idx],\n",
        "            \"BM25 점수\": bm25_scores[idx],\n",
        "            \"임베딩 유사도\": cosine_scores[idx],\n",
        "            \"결합 점수\": combined_scores[idx]\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Gradio 인터페이스 설정\n",
        "def gradio_interface(query, topic):\n",
        "    results = get_answer(query, topic)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return \"\\n\".join([f\"질문: {result['질문']}\\n답변: {result['답변']}\\nBM25 점수: {result['BM25 점수']}\\n임베딩 유사도: {result['임베딩 유사도']}\\n결합 점수: {result['결합 점수']}\"\n",
        "                       for result in results])\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"질문\", placeholder=\"여기에 질문을 입력하세요.\"),\n",
        "        gr.Dropdown(choices=list(data_files.keys()), label=\"주제 선택\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"학습 챗봇\",\n",
        "    description=\"과목을 선택하고 궁금한 걸 물어보세요!\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "0F42oPkNd2zZ",
        "outputId": "cfc854a7-4416-4b52-a9d0-5d7c26eb91bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ea4502cf1ba1161d7d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ea4502cf1ba1161d7d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 앙상블 모델 Gradio 챗봇 구현 (pkl 파일로 속도 높인 버전)"
      ],
      "metadata": {
        "id": "Q9uwWI7Dg2iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 데이터 파일 경로 설정\n",
        "data_files = {\n",
        "    '국어': 'korean_data.json',\n",
        "    '수학': 'math_data.json',\n",
        "    '영어': 'english_data.json',\n",
        "    '사회역사': 'social_data.json',\n",
        "    '과학': 'science_data.json'\n",
        "}\n",
        "\n",
        "# 형태소 분석기와 임베딩 모델 로드\n",
        "okt = Okt()\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "def save_pickle_file(topic):\n",
        "    # 선택된 주제에 맞는 데이터 파일 로드\n",
        "    file_path = data_files.get(topic)\n",
        "    if not file_path:\n",
        "        print(f\"{topic}의 데이터 파일을 찾을 수 없습니다.\")\n",
        "        return\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    questions = [item['instruction'] for item in data]\n",
        "    answers = [item['output'] for item in data]\n",
        "\n",
        "    # 형태소 분석을 통한 한국어 토큰화\n",
        "    tokenized_answers = [okt.morphs(answer) for answer in answers]\n",
        "\n",
        "    # BM25 모델 생성\n",
        "    bm25 = BM25Okapi(tokenized_answers, k1=1.5, b=0.75)\n",
        "\n",
        "    # 답변 임베딩 생성\n",
        "    answer_embeddings = model.encode(answers, convert_to_tensor=True)\n",
        "\n",
        "    # 피클 파일로 저장\n",
        "    pickle_file_path = f\"{topic}_data.pkl\"\n",
        "    with open(pickle_file_path, 'wb') as f:\n",
        "        pickle.dump((questions, answers, bm25, answer_embeddings), f)\n",
        "\n",
        "    print(f\"{topic}의 피클 파일이 저장되었습니다: {pickle_file_path}\")\n",
        "\n",
        "# 모든 주제에 대해 피클 파일 저장\n",
        "for topic in data_files.keys():\n",
        "    save_pickle_file(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UXakIgnePmU",
        "outputId": "aea2aa22-6dbd-401b-d34b-be2f630b9f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "국어의 피클 파일이 저장되었습니다: 국어_data.pkl\n",
            "수학의 피클 파일이 저장되었습니다: 수학_data.pkl\n",
            "영어의 피클 파일이 저장되었습니다: 영어_data.pkl\n",
            "사회역사의 피클 파일이 저장되었습니다: 사회역사_data.pkl\n",
            "과학의 피클 파일이 저장되었습니다: 과학_data.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from konlpy.tag import Okt, Komoran\n",
        "from functools import lru_cache\n",
        "\n",
        "# 형태소 분석기와 임베딩 모델 로드\n",
        "# okt = Okt()\n",
        "komoran = Komoran()\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def load_data(topic):\n",
        "    pickle_file_path = f\"{topic}_data.pkl\"\n",
        "    print(f\"Trying to load: {pickle_file_path}\")\n",
        "    try:\n",
        "        with open(pickle_file_path, 'rb') as f:\n",
        "            questions, answers, bm25, answer_embeddings = pickle.load(f)\n",
        "        return questions, answers, bm25, answer_embeddings\n",
        "    except FileNotFoundError:\n",
        "        return [], [], [], []\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return [], [], [], []\n",
        "def get_answer(topic, query):\n",
        "    questions, answers, bm25, answer_embeddings = load_data(topic)\n",
        "    if not questions:\n",
        "        return \"데이터를 로드할 수 없습니다. 주제를 선택하세요.\"\n",
        "\n",
        "    tokenized_answers = [komoran.morphs(answer) for answer in answers]\n",
        "    tokenized_query = komoran.morphs(query)\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # 임베딩 검색\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    cosine_scores = util.pytorch_cos_sim(query_embedding, answer_embeddings).squeeze().tolist()\n",
        "\n",
        "    # 점수 결합\n",
        "    alpha = 0.5\n",
        "    combined_scores = [(alpha * bm25_score + (1 - alpha) * cosine_score)\n",
        "                       for bm25_score, cosine_score in zip(bm25_scores, cosine_scores)]\n",
        "\n",
        "    # 상위 3개의 결과 인덱스 찾기\n",
        "    top_3_indices = np.argsort(combined_scores)[::-1][:3]\n",
        "\n",
        "    # 결과 구성\n",
        "    results = []\n",
        "    for idx in top_3_indices:\n",
        "        results.append({\n",
        "            \"질문\": questions[idx],\n",
        "            \"답변\": answers[idx],\n",
        "            \"BM25 점수\": bm25_scores[idx],\n",
        "            \"임베딩 유사도\": cosine_scores[idx],\n",
        "            \"결합 점수\": combined_scores[idx]\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Gradio 인터페이스 설정\n",
        "def gradio_interface(topic, query):\n",
        "    results = get_answer(topic, query)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return \"\\n\".join([f\"질문: {result['질문']}\\n답변: {result['답변']}\\nBM25 점수: {result['BM25 점수']}\\n임베딩 유사도: {result['임베딩 유사도']}\\n결합 점수: {result['결합 점수']}\"\n",
        "                       for result in results])\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=list(data_files.keys()), label=\"과목 선택\"),\n",
        "        gr.Textbox(label=\"질문\", placeholder=\"여기에 질문을 입력하세요.\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"학습 챗봇\",\n",
        "    description=\"과목을 선택하고 궁금한 걸 물어보세요!\"\n",
        ")\n",
        "\n",
        "iface.launch(debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "EtcsoluQeViz",
        "outputId": "39d79953-23f9-4877-9eee-5936dd3df081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://ab0e6a8176289b2143.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ab0e6a8176289b2143.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to load: 수학_data.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(io.BytesIO(b))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://ea4502cf1ba1161d7d.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://ab0e6a8176289b2143.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}